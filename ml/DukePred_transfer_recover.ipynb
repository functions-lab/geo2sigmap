{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "# start a new wandb run to track this script\n",
    "\n",
    "base_dir = \"DukePred_Transfer_Result\"\n",
    "\n",
    "raw_data_dir = \"DukePred_Transfer_Result/S\"\n",
    "state_dict_1st_UNet = \"../data/synthetic/checkpoint_epoch58.pth\"\n",
    "state_dict_2nd_UNet = \"../data/synthetic/checkpoint_epoch112.pth\"\n",
    "\n",
    "SS_NUM = 200\n",
    "USE_WANDB = False\n",
    "SHOW = True\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Test_Result\",     \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"Version\": \"Michael's\",\n",
    "            \"state_dict_1st_UNet\" :state_dict_1st_UNet,\n",
    "            \"state_dict_2nd_UNet\" : state_dict_2nd_UNet,\n",
    "            \"SS_NUM\": SS_NUM\n",
    "        }\n",
    "    )\n",
    "\n",
    "    raw_data_at = wandb.Artifact(artifact_dir, type=\"raw_data\")\n",
    "    raw_data_at.add_dir(artifact_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"rmse using transfer UNet (cascade)\n",
    "current issue: need to change a number in .forward of the UNet code when changing from \n",
    "UNet with isotropic cm to UNet with directional cm since the number of input channels \n",
    "are different. Temporary solution: run iso_UNet, save the output image, change the .forward \n",
    "function's number (2 to 3 or the other way round), restart kernel, run directional UNet. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The above comment no longer applies\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This is code for plotting the coverage maps as predicted by the isotropic and directional U-Nets.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage, datasets\n",
    "from scipy.constants import speed_of_light\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from unet.unet_model_rt import UNet\n",
    "from utils.utils import plot_img_and_mask\n",
    "\n",
    "from CBRSUtils import CBRSUtils as cu\n",
    "\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "b_map_path_dict = {'C': \"/home/yl826/3DPathLoss/nc_raytracing/POC/res_plane_duke_PCI6_7/Bl_building_npy/0_c8681783-aa00-46e7-92b2-ca15c5beca67.npy\",\n",
    "                  'D': \"/home/yl826/3DPathLoss/nc_raytracing/POC/res_plane_duke_PCI6_7/Bl_building_npy/0_c8681783-aa00-46e7-92b2-ca15c5beca67.npy\",\n",
    "                  'A': \"/home/yl826/3DPathLoss/nc_raytracing/POC/res_plane_duke_PCI6_7/Bl_building_npy/0_c8681783-aa00-46e7-92b2-ca15c5beca67.npy\",\n",
    "                  'B': \"/home/yl826/3DPathLoss/nc_raytracing/POC/res_plane_duke_PCI6_7/Bl_building_npy/0_c8681783-aa00-46e7-92b2-ca15c5beca67.npy\",\n",
    "                  'E': \"/home/yl826/3DPathLoss/nc_raytracing/POC/0_78d03890-be26-47f8-8b81-8485f0d19a83.npy\",\n",
    "                  'F': \"/home/yl826/3DPathLoss/nc_raytracing/POC/0_78d03890-be26-47f8-8b81-8485f0d19a83.npy\"}\n",
    "name_dict = {'C': \"0_c8681783-aa00-46e7-92b2-ca15c5beca67.npy\",\n",
    "                  'D': \"0_c8681783-aa00-46e7-92b2-ca15c5beca67.npy\",\n",
    "                  'A': \"0_c8681783-aa00-46e7-92b2-ca15c5beca67.npy\",\n",
    "                  'B': \"0_c8681783-aa00-46e7-92b2-ca15c5beca67.npy\",\n",
    "                  'E': \"0_78d03890-be26-47f8-8b81-8485f0d19a83.npy\",\n",
    "                  'F': \"0_78d03890-be26-47f8-8b81-8485f0d19a83.npy\"}\n",
    "\n",
    "sionna_cm_base = '/home/yl826/3DPathLoss/nc_raytracing/xml_to_heatmap/xml_to_heatmap_plotting/cm_npy_different_polarisations/'\n",
    "sionna_cm_path_dict = {'E': sionna_cm_base + 'PCI=20,Rx_pol=H,Tx_pol=cross.npy', \n",
    "                       'F': sionna_cm_base + 'PCI=20,Rx_pol=H,Tx_pol=cross.npy', \n",
    "                       'C': sionna_cm_base + 'PCI=6,Rx_pol=H,Tx_pol=cross.npy', \n",
    "                       'D': sionna_cm_base + 'PCI=6,Rx_pol=H,Tx_pol=cross.npy',\n",
    "                       'A': sionna_cm_base + 'PCI=3,Rx_pol=H,Tx_pol=cross.npy',\n",
    "                       'B': sionna_cm_base + 'PCI=3,Rx_pol=H,Tx_pol=cross.npy'}\n",
    "\n",
    "# lat lon pairs, antenna location\n",
    "\n",
    "lat_lon_center_dict = {'C': (35.999186254897836, -78.9397858960648),\n",
    "                            'D': (35.999186254897836, -78.9397858960648),\n",
    "                            'A': (35.999186254897836, -78.9397858960648),\n",
    "                            'B': (35.999186254897836, -78.9397858960648),\n",
    "                            'E':  (36.00307128210248, -78.93706070613855),\n",
    "                            'F':  (36.00307128210248, -78.93706070613855)}\n",
    "\n",
    "lat_lon_top_left_dict = {'C': (36.001483241320656, -78.94273733701348),\n",
    "                             'D': (36.001483241320656, -78.94273733701348),\n",
    "                             'A': (36.001483241320656, -78.94273733701348),\n",
    "                             'B': (36.001483241320656, -78.94273733701348),\n",
    "                             'E': (36.005429554748495, -78.93998431793196), \n",
    "                             'F': (36.005429554748495, -78.93998431793196)}  # lat, lon pairs for top left corner\n",
    "\n",
    "\n",
    "\n",
    "device_sql_dic = [\"Samsung\", \"pixel\", \"rpi\"]\n",
    "\n",
    "def checkNmkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(path)\n",
    "\n",
    "pci_azimuth_dict = {\n",
    "     'F': 216,\n",
    "     'E': 216,\n",
    "     'D': 30,\n",
    "     'C': 30,\n",
    "     'B': 212,\n",
    "     'A': 212\n",
    "\n",
    "}\n",
    "\n",
    "measurement_data = pd.read_csv(\"../data/measurement/masked_cleaned_view.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m PCI_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(lat_lon_top_left_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      4\u001b[0m random_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdevice_sql_dic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()):\n\u001b[1;32m      7\u001b[0m     artifact_dir \u001b[38;5;241m=\u001b[39m base_dir \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m device_name\n\u001b[1;32m      8\u001b[0m     artifact_dir_2nd \u001b[38;5;241m=\u001b[39m artifact_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2nd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "db_cache_res = {}\n",
    "\n",
    "PCI_list = list(lat_lon_top_left_dict.keys())\n",
    "random_index = 0\n",
    "\n",
    "for device_name in list(device_sql_dic.keys()):\n",
    "    artifact_dir = base_dir +\"/\" + device_name\n",
    "    artifact_dir_2nd = artifact_dir + \"/\" + \"2nd\"\n",
    "    artifact_dir_fitted = artifact_dir + \"/\" + \"fitted\"\n",
    "    checkNmkdir(artifact_dir)\n",
    "    checkNmkdir(artifact_dir_2nd)\n",
    "    checkNmkdir(artifact_dir_fitted)\n",
    "\n",
    "    \n",
    "    for which_PCI in PCI_list:\n",
    "\n",
    "        \n",
    "\n",
    "        print('Random_index: %d, SS_NUM: %d, PCI: %s' % (random_index, SS_NUM, which_PCI))\n",
    "        save_iso_output_name = 'PCI=%s_iso_UNet_out' % (which_PCI)\n",
    "\n",
    "        def log(log_dict, commit_flag=False):\n",
    "            if USE_WANDB:\n",
    "                wandb.log(log_dict,commit=commit_flag) \n",
    "\n",
    "        \n",
    "        df_SigCapDetails = measurement_data[(measurement_data['mPci'] == which_PCI) & (measurement_data['device'] == device_name)]\n",
    "        print(df_SigCapDetails)\n",
    "\n",
    "\n",
    "        if SHOW:\n",
    "            print(df_SigCapDetails['latitude'][0:5])\n",
    "            print(df_SigCapDetails['longitude'][0:5])\n",
    "            print((df_SigCapDetails['rsrp'][::100]))\n",
    "\n",
    "        log({\"Total Measurement Points\": len(df_SigCapDetails[\"latitude\"])})\n",
    "        # 3.Load data\n",
    "        # first, get idx_uuid.npy (building map name)\n",
    "        name = name_dict[which_PCI]\n",
    "\n",
    "        # load building map and downsample\n",
    "        building_height_ori = np.load(b_map_path_dict[which_PCI])\n",
    "        building_height_ori = building_height_ori[0:512, 0:512]\n",
    "        building_height_arr = building_height_ori[::4, ::4]\n",
    "\n",
    "\n",
    "        name_splited = name.split(\"_\")\n",
    "        file_name_id_part = name_splited[0]  # idxuuid\n",
    "        tx_x = 256\n",
    "        tx_y = 256\n",
    "        tx_position = [tx_x // 4, tx_y // 4]\n",
    "        tx_height = 24\n",
    "        # tx_height = np.max(building_height_arr)\n",
    "        if SHOW:\n",
    "            print('tx_height', int(tx_height))\n",
    "        log({\"tx_height\": int(tx_height)})\n",
    "\n",
    "        distance = np.arange(0, 1450, 1)\n",
    "\n",
    "        # lookup table for path_loss_res, resolution=1 meter\n",
    "        path_loss_res =  cu.pathloss_38901(distance,3.64, h_bs=int(tx_height), h_ut=2)\n",
    "\n",
    "        # generate list of tuples of (numpy_row, numpy_col, rsrp) by converting lat,lon into \n",
    "        # local coordinates. 'list of tuples' is represented by np.array\n",
    "        measurement = np.empty([len(df_SigCapDetails[\"latitude\"]), 3])\n",
    "        measurement[:, 0] = df_SigCapDetails[\"latitude\"]\n",
    "        measurement[:, 1] = df_SigCapDetails[\"longitude\"]\n",
    "        measurement[:, 2] = df_SigCapDetails[\"rsrp\"]\n",
    "\n",
    "        for idx, point in enumerate(measurement):\n",
    "            row, col = cu.gps2local(measurement[idx,0], measurement[idx,1], \n",
    "                                    top_left_latitude=lat_lon_top_left_dict[which_PCI][0],\n",
    "                                    top_left_longitude=lat_lon_top_left_dict[which_PCI][1])\n",
    "            measurement[idx, 0] = row\n",
    "            measurement[idx, 1] = col\n",
    "\n",
    "        # get sparse_ss\n",
    "        ground_truth_list, sparse_ss_arr = np.split(np.random.default_rng(seed=42).permutation(measurement), [len(measurement)-200])\n",
    "        ground_truth_list = measurement\n",
    "\n",
    "        sparse_ss_arr[:, 0] = sparse_ss_arr[:, 0]/4.0\n",
    "        sparse_ss_arr[:, 1] = sparse_ss_arr[:, 1]/4.0\n",
    "\n",
    "        increment_permutations = np.array(list(itertools.permutations([-1, 0, 1]))).reshape(-1, 1)\n",
    "\n",
    "        def col_row_for_no_building(row_, col_, bh_arr=building_height_arr, \n",
    "                                    increment=increment_permutations):\n",
    "            \"\"\"\n",
    "            :param row_: index that has height != 0\n",
    "            :param col_: index that has height != 0\n",
    "            :param bh_arr: building height array\n",
    "            :param increment: tuples for incrementing [row,col]\n",
    "            \"\"\"\n",
    "            for iii in range(3):\n",
    "                for jjj in range(3):\n",
    "                    try: \n",
    "                        height_ = bh_arr[row_+increment[2*iii][jjj], col_+increment[2*iii+1][jjj]]\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    if height_ == 0:\n",
    "                        return row_+increment[2*iii][jjj], col_+increment[2*iii+1][jjj]\n",
    "            # cannot find no building area in 3x3 region\n",
    "            return None, None\n",
    "        if SHOW:\n",
    "            print(\"# of points for point-net: \", len(sparse_ss_arr))\n",
    "            print(\"# of points for ground_truth: \", len(ground_truth_list))\n",
    "\n",
    "        log({\"# of points for S\": len(sparse_ss_arr)})\n",
    "        log({\"# of points for GT\": len(ground_truth_list)})\n",
    "\n",
    "        ground_truth_tmp =  np.empty((128, 128), dtype=object)\n",
    "        count_num_no_building = 0\n",
    "        \n",
    "        print(len(ground_truth_list))\n",
    "        for point in ground_truth_list:\n",
    "            row = int(point[0]/4)\n",
    "            col = int(point[1]/4)\n",
    "            if row >=128 or col >= 128 or row < 0 or col < 0:\n",
    "                #print(\"{}:{}\".format(row,col))\n",
    "                continue\n",
    "            #valid_num_mea_dic[device_name][which_PCI] = valid_num_mea_dic[device_name][which_PCI] +1\n",
    "            if building_height_arr[row,col] != 0:\n",
    "                row, col = col_row_for_no_building(row, col)  # attempts to update row, col \n",
    "                if row is None or col is None:  # didn't find good row, col\n",
    "                    count_num_no_building += 1\n",
    "                    continue\n",
    "            if ground_truth_tmp[row, col] is None:\n",
    "                ground_truth_tmp[row, col] = np.array(point[2])\n",
    "            else:\n",
    "                ground_truth_tmp[row][col] = np.append(ground_truth_tmp[row][col], point[2])\n",
    "                \n",
    "        if SHOW:\n",
    "            print(\"number of points where there's a building\", count_num_no_building)\n",
    "        log({\"number of points where there's a building\": count_num_no_building})\n",
    "\n",
    "        ground_truth_arr = np.empty((128, 128))\n",
    "        for row in range(128):\n",
    "            for col in range(128):\n",
    "                if ground_truth_tmp[row, col] is None:\n",
    "                    ground_truth_arr[row][col] = -160\n",
    "                    \n",
    "                else:\n",
    "                    ground_truth_arr[row][col] = ground_truth_tmp[row,col].mean()\n",
    "                    ground_truth_arr[row][col] = np.median(ground_truth_tmp[row,col])\n",
    "                    ground_truth_arr[row][col] = 10 * np.log10( ( 10 ** ( ground_truth_tmp[row,col]/ 10)  ).mean() )\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"Sanity Chrck\")\n",
    "        print(np.count_nonzero(ground_truth_arr != -160))\n",
    "        if SHOW:\n",
    "            plt.imshow(ground_truth_arr)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        '''\n",
    "        Random pick x points and then compute the mean of it\n",
    "        '''      \n",
    "        ss_map = np.full((128, 128), -160)\n",
    "        tmp_ss_list = []\n",
    "        count = 0\n",
    "        while count < 100:\n",
    "                    \n",
    "            xx = random.randint(0, 127)\n",
    "            yy = random.randint(0, 127)\n",
    "            if ground_truth_arr[xx][yy] != -160 and ss_map[xx][yy] == -160:\n",
    "                count+=1\n",
    "                ss_map[xx][yy] = ground_truth_arr[xx][yy]\n",
    "                tmp_ss_list.append(ground_truth_arr[xx][yy])\n",
    "        GT_mean =  sum(tmp_ss_list) / len(tmp_ss_list)\n",
    "        '''\n",
    "        Generate the fitted Sionna Result\n",
    "        '''  \n",
    "        sionna_cm = 10 * np.log10(np.flip(np.load(sionna_cm_path_dict[which_PCI]),0))\n",
    "        sionna_cm = np.where(building_height_arr!=0, -160, sionna_cm)\n",
    "        sionna_cm = np.where(np.isinf(sionna_cm), -160, sionna_cm)\n",
    "        \n",
    "\n",
    "        np.save('%s/%s,RT.npy'% (artifact_dir, which_PCI),sionna_cm)\n",
    "        np.save('%s/%s,RT.npy'% (artifact_dir_fitted, which_PCI),sionna_cm + (GT_mean -  np.mean(np.ma.masked_where(ground_truth_arr==-160, sionna_cm))))\n",
    "        \n",
    "        # Construct the TX position channel\n",
    "        tx_position_channel = np.full((128, 128), 0, dtype=int)\n",
    "        tx_position_channel[tx_position[1]][tx_position[0]] = tx_height\n",
    "\n",
    "\n",
    "        path_loss_heat_map = np.full((128, 128), 0, dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "        for row in range(path_loss_heat_map.shape[0]):\n",
    "            for col in range(path_loss_heat_map.shape[1]):\n",
    "                # Compute the distance between pixel and tx\n",
    "                dist = math.sqrt((tx_position[1]*4 - row*4)**2 + (tx_position[0]*4 - col*4)**2)\n",
    "                path_loss_heat_map[row][col] =  -1 * path_loss_res[int(dist)]\n",
    "        ### net for isotropic antenna\n",
    "        np.save('%s/%s,ITM.npy'% (artifact_dir, which_PCI), path_loss_heat_map)\n",
    "        np.save('%s/%s,ITM.npy'% (artifact_dir_fitted, which_PCI),path_loss_heat_map + (GT_mean -  np.mean(np.ma.masked_where(ground_truth_arr==-160, path_loss_heat_map))))\n",
    "        \n",
    "\n",
    "        net_iso = UNet(n_channels=2, n_classes=1, bilinear=False, pathloss=True)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        net_iso.to(device=device)\n",
    "\n",
    "\n",
    "        # 2.Load state dict\n",
    "        #state_dict = torch.load(state_dict_1st_UNet, map_location=device)\n",
    "        state_dict = torch.load(state_dict_1st_UNet, map_location=device)\n",
    "\n",
    "        net_iso.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "        # Since right now GT.size is 100*100 and other two size is 1000 * 1000, just check the input.\n",
    "        # assert building_height_arr.shape == terrain_height_aimport loggin\n",
    "        combined_input = np.zeros((4, 128, 128), dtype=float)\n",
    "\n",
    "        # Combine all the channels together\n",
    "        combined_input[0,:, :] = np.copy(building_height_arr)\n",
    "        \n",
    "        #\n",
    "        combined_input[1,:, :] = tx_position_channel\n",
    "        combined_input[2,:, :] = path_loss_heat_map\n",
    "\n",
    "\n",
    "        batch = {\n",
    "                    'combined_input': torch.as_tensor(combined_input.copy()).float().contiguous(),\n",
    "                    'ground_truth': torch.as_tensor(ground_truth_arr.copy()).long().contiguous(),\n",
    "                    'file_name': name,\n",
    "                    'sparse_ss': torch.as_tensor(sparse_ss_arr.copy()).float().contiguous()\n",
    "                }\n",
    "\n",
    "\n",
    "        # Eval\n",
    "        images, ground_truth = batch['combined_input'], batch['ground_truth']\n",
    "        sparse_ss = batch['sparse_ss']\n",
    "\n",
    "        # Since we directly load the data by manual, add batch dimension to the front of data\n",
    "        images = images[None,:, :, :]\n",
    "        sparse_ss = sparse_ss[None,:, :]\n",
    "\n",
    "\n",
    "        net_iso.eval()\n",
    "\n",
    "        images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "        ground_truth = ground_truth.to(device=device, dtype=torch.long)\n",
    "        sparse_ss = sparse_ss.to(device=device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = net_iso(images, sparse_ss).squeeze().cpu()\n",
    "\n",
    "        output_iso_UNet = output.squeeze().cpu().numpy()\n",
    "        #np.save('%s/%d,%d,%d,1stUNet.npy'% (artifact_dir, random_index, SS_NUM, which_PCI), output_iso_UNet)\n",
    "\n",
    "        np.save('%s/%s,PLNet.npy'% (artifact_dir, which_PCI), output_iso_UNet)\n",
    "        np.save('%s/%s,PLNet.npy'% (artifact_dir_fitted, which_PCI), output_iso_UNet + (GT_mean -  np.mean(np.ma.masked_where(ground_truth_arr==-160, output_iso_UNet))))\n",
    "\n",
    "        if SHOW or True:\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            \n",
    "            image = ax.imshow(output_iso_UNet, interpolation='none', rasterized=True)\n",
    "\n",
    "            ax.set_title('Iso UNet', size=22)\n",
    "            plt.axis(\"off\")\n",
    "            fig.colorbar(image, ax=ax, orientation='vertical')\n",
    "            fig.tight_layout()\n",
    "\n",
    "            log({\"1st UNet Predict\":fig})\n",
    "        plt.savefig('output_iso_UNet_PCI%s.pdf' % which_PCI, dpi=500, bbox_inches='tight')\n",
    "\n",
    "        np.save(save_iso_output_name, output_iso_UNet)\n",
    "        # UNet for tr38901\n",
    "\n",
    "        # 1.Create the Model\n",
    "        net = UNet(n_channels=3, n_classes=1, bilinear=False, pathloss=False)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        net.to(device=device)\n",
    "\n",
    "\n",
    "        # 2.Load state dict\n",
    "        #state_dict = torch.load(\"../Pytorch-UNet-master/checkpoints_transfer/checkpoint_epoch21.pth\", map_location=device)\n",
    "        state_dict = torch.load(state_dict_2nd_UNet, map_location=device)\n",
    "\n",
    "        net.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Since right now GT.size is 100*100 and other two size is 1000 * 1000, just check the input.\n",
    "        # assert building_height_arr.shape == terrain_height_aimport loggin\n",
    "        combined_input = np.zeros((3, 128, 128), dtype=float)\n",
    "\n",
    "        # Combine all the channels together\n",
    "        combined_input[0,:, :] = building_height_arr\n",
    "        ss_map = np.full((128, 128), -160)\n",
    "\n",
    "        res = []\n",
    "        ss_map = np.full((128, 128), -160)\n",
    "        count = 0\n",
    "        while count < SS_NUM:\n",
    "            \n",
    "            xx = random.randint(0, 127)\n",
    "            yy = random.randint(0, 127)\n",
    "            if ground_truth_arr[xx][yy] != -160 and ss_map[xx][yy] == -160:\n",
    "                count+=1\n",
    "                ss_map[xx][yy] = ground_truth_arr[xx][yy]\n",
    "\n",
    "        combined_input[1,:, :] = ss_map\n",
    "        if SHOW:\n",
    "            plt.imshow(ss_map)\n",
    "            plt.colorbar()\n",
    "            plt.close()\n",
    "        combined_input[2,:, :] = np.load(save_iso_output_name + '.npy') \n",
    "        #combined_input[2,:, :] = output_iso_UNet\n",
    "\n",
    "        batch = {\n",
    "                    'combined_input': torch.as_tensor(combined_input.copy()).float().contiguous(),\n",
    "                    'ground_truth': torch.as_tensor(ground_truth_arr.copy()).long().contiguous(),\n",
    "                    'file_name': name,\n",
    "                    'sparse_ss': torch.as_tensor(sparse_ss_arr.copy()).float().contiguous()\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 4. Eval\n",
    "        images, ground_truth = batch['combined_input'], batch['ground_truth']\n",
    "        sparse_ss = batch['sparse_ss']\n",
    "\n",
    "        # Since we directly load the data by manual, add batch dimension to the front of data\n",
    "        images = images[None,:, :, :]\n",
    "        sparse_ss = sparse_ss[None,:, :]\n",
    "\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "        ground_truth = ground_truth.to(device=device, dtype=torch.long)\n",
    "        sparse_ss = sparse_ss.to(device=device, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = net(images, sparse_ss).cpu()\n",
    "\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "\n",
    "        #np.save('%s/%d,2ndUNet.npy'% (artifact_dir, which_PCI), output)\n",
    "        np.save('%s/%d,%d,%s,2ndUNet.npy'% (artifact_dir_2nd, 0, 200, which_PCI), output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def generate_pathloss_heatmap(model_name, h_bs, h_ut, freq, shape, tx_position):\n",
    "            model_dict = {\n",
    "            \"ericsson\": cu.pathloss_friis_free_space_model,\n",
    "            \"friis\": cu.pathloss_ericsson_model,\n",
    "            }\n",
    "            \n",
    "            func = model_dict.get(model_name, None)\n",
    "            assert func is not None, \"Model name not found! Try ericsson or friis\"\n",
    "            assert freq > 10, \"Freq in MHz! Not GHz\"\n",
    "            \n",
    "            \n",
    "            path_loss_heat_map = np.full(shape, 0, dtype=float)\n",
    "            \n",
    "            for row in range(path_loss_heat_map.shape[0]):\n",
    "                for col in range(path_loss_heat_map.shape[1]):\n",
    "                    \n",
    "                    dist = math.sqrt((tx_position[0]*4 - row*4)**2 + (tx_position[1]*4 - col*4)**2)\n",
    "                    \n",
    "                    path_loss_heat_map[row][col] =  -1 * func(dist/1000, h_bs, h_ut, freq )\n",
    "            return path_loss_heat_map\n",
    "        ericsson_pl = generate_pathloss_heatmap(\"ericsson\",int(tx_height), 2, 3640,  (128,128), tx_position )\n",
    "\n",
    "        if SHOW:\n",
    "            plt.imshow(ericsson_pl)\n",
    "            plt.title(\"Ericsson\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "        \n",
    "\n",
    "        np.save('%s/%s,ericsson.npy'% (artifact_dir, which_PCI), ericsson_pl)\n",
    "        \n",
    "        np.save('%s/%s,ericsson.npy'% (artifact_dir_fitted, which_PCI),ericsson_pl + (GT_mean -  np.mean(np.ma.masked_where(ground_truth_arr==-160, ericsson_pl))))\n",
    "\n",
    "        #generate_linear_regression_result(ericsson_pl , ground_truth_arr, \"ori\")\n",
    "        # print(GT_mean )\n",
    "        # print(np.ma.masked_where(building_height_arr!=0, ericsson_pl))\n",
    "        # plt.imshow(np.ma.masked_where(building_height_arr!=0, ericsson_pl))\n",
    "        # plt.show()\n",
    "        # print(np.mean(np.ma.masked_where(building_height_arr!=0, ericsson_pl)))\n",
    "        #generate_linear_regression_result(ericsson_pl + (GT_mean -  np.mean(np.ma.masked_where(building_height_arr!=0, ericsson_pl))), ground_truth_arr, \"fitted\")\n",
    "\n",
    "\n",
    "        friis_pl = generate_pathloss_heatmap(\"friis\",int(tx_height), 2, 3640, (128,128), tx_position )\n",
    "    \n",
    "        np.save('%s/%s,friis.npy'% (artifact_dir, which_PCI), friis_pl)\n",
    "        np.save('%s/%s,friis.npy'% (artifact_dir_fitted, which_PCI),friis_pl + (GT_mean -  np.mean(np.ma.masked_where(ground_truth_arr==-160, friis_pl))))\n",
    "        if SHOW:\n",
    "            plt.imshow(friis_pl)\n",
    "            plt.title(\"Friis\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "        if SHOW:\n",
    "            ericsson_coef, ericsson_intercept, ericsson_ori_rmse, ericsson_fitted_rmse = cu.generate_linear_regression_result(ericsson_pl,ground_truth_arr,\"Ericsson\")\n",
    "\n",
    "            log({\"Ericsson RMSE\":ericsson_ori_rmse})\n",
    "\n",
    "            friis_coef, friis_intercept, friis_ori_rmse, friis_fitted_rmse = cu.generate_linear_regression_result(friis_pl,ground_truth_arr,\"friis\")\n",
    "            log({\"friis RMSE\":friis_ori_rmse})\n",
    "\n",
    "\n",
    "\n",
    "        def generate_linear_regression_result(prediction, ground_truth, title=\"\", print_result=True,SHOW=False):\n",
    "            \"\"\"\n",
    "            :param prediction:  predicted value\n",
    "            :param ground_truth: ground truth value\n",
    "            :param title:\n",
    "            :param print_result:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            assert ground_truth.shape == prediction.shape, \"ground_truth and prediction shape mismatch \"\n",
    "        \n",
    "            print()\n",
    "            print(\"++++++++++++++\", title, \"++++++++++++++\")\n",
    "            X = []\n",
    "            y = []\n",
    "            for row in range(prediction.shape[0]):\n",
    "                for col in range(prediction.shape[1]):\n",
    "                    if ground_truth[row, col] != -160:\n",
    "                        X.append(prediction[row, col])\n",
    "                        y.append(ground_truth[row, col])\n",
    "            X = np.array(X).reshape(-1, 1)\n",
    "            tmp_res = []\n",
    "            tmp_x = np.arange(0.05,1.0,0.05)\n",
    "            ori_rmse = mean_squared_error(y, X, squared=False)\n",
    "        \n",
    "            print(\"original without fitting rmse \", ori_rmse)\n",
    "            log({title + \" RMSE\":ori_rmse})\n",
    "\n",
    "            for i in np.arange(0.05,1.0,0.05):\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i, random_state = 42)\n",
    "                reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "        #         print(\"score\", reg.score(X, y))\n",
    "        #         print(\"coef\", reg.coef_[0])\n",
    "\n",
    "        #         print(\"intercept\", reg.intercept_)\n",
    "\n",
    "                ori_rmse = mean_squared_error(y, X, squared=False)\n",
    "                \n",
    "\n",
    "                fitted_rmse = mean_squared_error(y, X * reg.coef_ + reg.intercept_, squared=False)\n",
    "                #print(\"rmse after linear regression \", fitted_rmse)\n",
    "\n",
    "                ori_mae = mean_absolute_error(y, X)\n",
    "                fitted_mae = mean_absolute_error(y, X * reg.coef_ + reg.intercept_)\n",
    "                # print(\"mae \", ori_mae)\n",
    "                # print(\"mae after linear regression \", fitted_rmse)\n",
    "                tmp_res.append(fitted_rmse)\n",
    "            if SHOW:\n",
    "                plt.plot(tmp_x, tmp_res)\n",
    "                plt.title(\"Test set ratio vs. RMSE\")\n",
    "                plt.xlabel('Test set ratio')\n",
    "                plt.ylabel('RMSE in dB')\n",
    "\n",
    "                plt.show()\n",
    "            \n",
    "            return reg.coef_[0], reg.intercept_, ori_rmse, fitted_mae\n",
    "            \n",
    "\n",
    "        if SHOW:\n",
    "            ml_coef, ml_intercept, ml_ori_rmse, ml_fitted_rmse = generate_linear_regression_result(output,ground_truth_arr,\"ML\")\n",
    "            pl_coef, pl_intercept, pl_ori_rmse, pl_fitted_rmse = generate_linear_regression_result(path_loss_heat_map,ground_truth_arr,\"PL\")\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            image = ax.imshow(output, interpolation='none', rasterized=True)\n",
    "\n",
    "            ax.set_title('Directional UNet', size=22)\n",
    "            plt.axis(\"off\")\n",
    "            fig.colorbar(image, ax=ax, orientation='vertical')\n",
    "            fig.tight_layout()\n",
    "            log({\"2nd UNet Predict\":fig})\n",
    "        # plt.savefig('output_directional_UNet_PCI%d.pdf' % which_PCI, dpi=500, bbox_inches='tight')\n",
    "        # base_path_sionna = '/home/yl826/3DPathLoss/nc_raytracing/'\n",
    "        # sionna_cm_path_dict = {20: 'cm_plane_duke_pci20_7e6_tr38901Tx/'\n",
    "        #                        }\n",
    "        # f_names_sig_map_idx_uuid = list(set([f.split('_')[0] + '_' + f.split('_')[1]\n",
    "        #                                     for f in os.listdir(base_path_sionna + \n",
    "        #                                                         sionna_cm_path_dict[which_PCI])\n",
    "        #                                 if os.path.isfile(base_path_sionna + \n",
    "        #                                                   sionna_cm_path_dict[which_PCI] + f)]))\n",
    "        # f_names = list(set([f for f in os.listdir(base_path_sionna + sionna_cm_path_dict[which_PCI])\n",
    "        #                                 if os.path.isfile(base_path_sionna + \n",
    "        #                                                   sionna_cm_path_dict[which_PCI] + f)]))\n",
    "        # assert len(f_names) == 1\n",
    "        # sionna_cm = 10 * np.log10(np.load(base_path_sionna + sionna_cm_path_dict[which_PCI] + f_names[0]))\n",
    "        # sionna_cm = np.nan_to_num(sionna_cm, copy=True, nan=-160, posinf=-160, neginf=-160)\n",
    "        # sionna_cm[sionna_cm < -160] = -160\n",
    "        # fig, ax = plt.subplots(1, 1)\n",
    "        # image = ax.imshow(sionna_cm, interpolation='none', rasterized=True)\n",
    "\n",
    "        # ax.set_title('Coverage Map', size=22)\n",
    "        # plt.axis(\"off\")\n",
    "        # fig.colorbar(image, ax=ax, orientation='vertical')\n",
    "        # fig.tight_layout()\n",
    "        # plt.savefig('coverage_map_PCI%d.pdf' % which_PCI, dpi=500, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if SHOW:\n",
    "            b_map_npy = np.load(b_map_path_dict[which_PCI])[0:512, 0:512]\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            image = ax.imshow(b_map_npy, interpolation='none', rasterized=True)\n",
    "\n",
    "            ax.set_title('Building Map', size=22)\n",
    "            fig.colorbar(image, ax=ax, orientation='vertical')\n",
    "            plt.axis(\"off\")\n",
    "            fig.tight_layout()\n",
    "            log({\"Building Map\":fig})\n",
    "            #plt.savefig('building_map_PCI%d.pdf' % which_PCI, dpi=500, bbox_inches='tight')\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            image = ax.imshow(ss_map, interpolation='none', rasterized=True)\n",
    "\n",
    "            ax.set_title('Sparse Signal Map', size=22)\n",
    "            plt.axis(\"off\")\n",
    "            fig.colorbar(image, ax=ax, orientation='vertical')\n",
    "            fig.tight_layout()\n",
    "            log({\"Sparse Signal Map\":fig})\n",
    "            #plt.savefig('ss_map_PCI%d.pdf' % which_PCI, dpi=500, bbox_inches='tight')\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            image = ax.imshow(ground_truth_arr, interpolation='none', rasterized=True)\n",
    "\n",
    "            ax.set_title('Ground Truth Measurements', size=22)\n",
    "            plt.axis(\"off\")\n",
    "            fig.colorbar(image, ax=ax, orientation='vertical')\n",
    "            fig.tight_layout()\n",
    "            log({\"Ground Truth Measurements\":fig})\n",
    "            #plt.savefig('ground_truth_map_PCI%d.pdf' % which_PCI, dpi=500, bbox_inches='tight')\n",
    "            # cu.gps2local(36.00247804455967, -78.94120458940182)\n",
    "\n",
    "            output_tmp = np.copy(output)\n",
    "            output_mask = (ground_truth_arr == -160)\n",
    "            result = ground_truth_arr - output_tmp\n",
    "            result[output_mask] = np.nan\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            image = ax.imshow(result , interpolation='none', rasterized=True)\n",
    "\n",
    "            ax.set_title('Ground Truth - 2nd UNet Predict ', size=22)\n",
    "            plt.axis(\"off\")\n",
    "            fig.colorbar(image, ax=ax, orientation='vertical')\n",
    "            fig.tight_layout()\n",
    "            log({\"Ground Truth - 2nd UNet Predict\":fig},True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
